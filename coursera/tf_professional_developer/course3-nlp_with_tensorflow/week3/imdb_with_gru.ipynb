{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMDB Reviews with a GRU\n",
    "*Also optional LSTM and ConvNet*\n",
    "\n",
    "* [Google Colab: For this exercise](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202d.ipynb#scrollTo=nHGYuU4jPYaj)\n",
    "* [Google Colab Solution](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP%20Course%20-%20Week%203%20Exercise%20Answer.ipynb)\n",
    "* [Google Colab: Sarcasm with Bidirectional LSTM](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202.ipynb#scrollTo=g9DC6dmLF8DC)\n",
    "* [Google Colab: Sarcasm with 1D Convolutional Layer](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202c.ipynb#scrollTo=g9DC6dmLF8DC)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# !pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Preprocessing\n",
    "\n",
    "We need to split the trainining and testing sentences and labels,\n",
    "but we also need to turn everything into Numpy arrays to more easily work with.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy(). I'm using 3.8 and didn't need this\n",
    "for s, l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "  \n",
    "for s, l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())\n",
    "  \n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tokenize the texts and turn them into padded, numerical sequences to be fed into the Neural Network \"\"\"\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16 # aka output_dim\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all '\nb'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Write a decode function so we can turn numerical sequences back into regular sentence.\n",
    "\n",
    "With this we can see the effects of our preprocessing before we feed it into the RNN.\n",
    "\"\"\"\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "# Print the first training review so we can see the diff between the padded and original.\n",
    "# Observe that the padded sentence is ?, is lowercase, and even has punctuation removed!\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 120, 16)           160000    \n_________________________________________________________________\nbidirectional (Bidirectional (None, 64)                9600      \n_________________________________________________________________\ndense (Dense)                (None, 6)                 390       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 7         \n=================================================================\nTotal params: 169,997\nTrainable params: 169,997\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Define our bidirectional GRU model \"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compile the model \"\"\"\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.5052 - accuracy: 0.7274 - val_loss: 0.3511 - val_accuracy: 0.8458\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.2987 - accuracy: 0.8775 - val_loss: 0.3604 - val_accuracy: 0.8488\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 29s 38ms/step - loss: 0.2344 - accuracy: 0.9092 - val_loss: 0.3930 - val_accuracy: 0.8406\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 31s 39ms/step - loss: 0.1893 - accuracy: 0.9280 - val_loss: 0.4084 - val_accuracy: 0.8364\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.1514 - accuracy: 0.9444 - val_loss: 0.4361 - val_accuracy: 0.8286\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.1209 - accuracy: 0.9566 - val_loss: 0.5064 - val_accuracy: 0.8202\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.0923 - accuracy: 0.9676 - val_loss: 0.6512 - val_accuracy: 0.8128\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 29s 36ms/step - loss: 0.0688 - accuracy: 0.9758 - val_loss: 0.6670 - val_accuracy: 0.8167\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 31s 40ms/step - loss: 0.0509 - accuracy: 0.9832 - val_loss: 0.6664 - val_accuracy: 0.8111\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0401 - accuracy: 0.9868 - val_loss: 0.8497 - val_accuracy: 0.8146\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.0310 - accuracy: 0.9897 - val_loss: 0.8844 - val_accuracy: 0.8098\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 1.0280 - val_accuracy: 0.8088\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0209 - accuracy: 0.9928 - val_loss: 1.0589 - val_accuracy: 0.8139\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 1.1746 - val_accuracy: 0.8119\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 1.0442 - val_accuracy: 0.8123\n",
      "Epoch 16/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0142 - accuracy: 0.9956 - val_loss: 1.1719 - val_accuracy: 0.8126\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0098 - accuracy: 0.9968 - val_loss: 1.1294 - val_accuracy: 0.8089\n",
      "Epoch 18/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 1.1285 - val_accuracy: 0.8085\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0123 - accuracy: 0.9962 - val_loss: 1.1852 - val_accuracy: 0.8106\n",
      "Epoch 20/50\n",
      "782/782 [==============================] - 30s 39ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 1.3280 - val_accuracy: 0.8063\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 1.3161 - val_accuracy: 0.8123\n",
      "Epoch 22/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 1.2678 - val_accuracy: 0.8068\n",
      "Epoch 23/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0072 - accuracy: 0.9974 - val_loss: 1.3505 - val_accuracy: 0.8062\n",
      "Epoch 24/50\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 1.4465 - val_accuracy: 0.8071\n",
      "Epoch 25/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0064 - accuracy: 0.9978 - val_loss: 1.4103 - val_accuracy: 0.8086\n",
      "Epoch 26/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 1.3533 - val_accuracy: 0.8056\n",
      "Epoch 27/50\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 1.3766 - val_accuracy: 0.8040\n",
      "Epoch 28/50\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 1.3872 - val_accuracy: 0.8127\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 1.4167 - val_accuracy: 0.8070\n",
      "Epoch 30/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.4954 - val_accuracy: 0.8087\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 9.3198e-05 - accuracy: 1.0000 - val_loss: 1.6351 - val_accuracy: 0.8090\n",
      "Epoch 32/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 3.4668e-05 - accuracy: 1.0000 - val_loss: 1.6992 - val_accuracy: 0.8096\n",
      "Epoch 33/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 2.1801e-05 - accuracy: 1.0000 - val_loss: 1.7638 - val_accuracy: 0.8096\n",
      "Epoch 34/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.4710e-05 - accuracy: 1.0000 - val_loss: 1.8290 - val_accuracy: 0.8096\n",
      "Epoch 35/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 9.9388e-06 - accuracy: 1.0000 - val_loss: 1.8926 - val_accuracy: 0.8093\n",
      "Epoch 36/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 6.6572e-06 - accuracy: 1.0000 - val_loss: 1.9626 - val_accuracy: 0.8095\n",
      "Epoch 37/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 4.4547e-06 - accuracy: 1.0000 - val_loss: 2.0291 - val_accuracy: 0.8092\n",
      "Epoch 38/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 2.9499e-06 - accuracy: 1.0000 - val_loss: 2.1016 - val_accuracy: 0.8094\n",
      "Epoch 39/50\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.9486e-06 - accuracy: 1.0000 - val_loss: 2.1725 - val_accuracy: 0.8095\n",
      "Epoch 40/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 1.2803e-06 - accuracy: 1.0000 - val_loss: 2.2474 - val_accuracy: 0.8100\n",
      "Epoch 41/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 8.3992e-07 - accuracy: 1.0000 - val_loss: 2.3174 - val_accuracy: 0.8097\n",
      "Epoch 42/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 5.4732e-07 - accuracy: 1.0000 - val_loss: 2.3951 - val_accuracy: 0.8103\n",
      "Epoch 43/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 3.5993e-07 - accuracy: 1.0000 - val_loss: 2.4638 - val_accuracy: 0.8098\n",
      "Epoch 44/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 2.3614e-07 - accuracy: 1.0000 - val_loss: 2.5394 - val_accuracy: 0.8104\n",
      "Epoch 45/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 1.5594e-07 - accuracy: 1.0000 - val_loss: 2.6091 - val_accuracy: 0.8106\n",
      "Epoch 46/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 1.0233e-07 - accuracy: 1.0000 - val_loss: 2.6855 - val_accuracy: 0.8107\n",
      "Epoch 47/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 6.8037e-08 - accuracy: 1.0000 - val_loss: 2.7533 - val_accuracy: 0.8103\n",
      "Epoch 48/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 4.5354e-08 - accuracy: 1.0000 - val_loss: 2.8242 - val_accuracy: 0.8104\n",
      "Epoch 49/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 3.0416e-08 - accuracy: 1.0000 - val_loss: 2.8900 - val_accuracy: 0.8105\n",
      "Epoch 50/50\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 2.0582e-08 - accuracy: 1.0000 - val_loss: 2.9527 - val_accuracy: 0.8105\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train the model \"\"\"\n",
    "num_epochs = 50\n",
    "history = model.fit(\n",
    "    padded,                # x = padded training sequences\n",
    "    training_labels_final, # y = training labels as numpy arrays\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(testing_padded, testing_labels_final)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize the training performance \"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model definition with LSTM \"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Model definition with Conv1D \"\"\"\n",
    "# Model Definition with Conv1D\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ]
}